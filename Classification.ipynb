{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Классификация\n",
    "В данном уроке мы рассмотрим одну из самых популярных задач машинного обучения - классификацию.\n",
    "\n",
    "У нас есть некоторые данные. Эти данные следует отнести к заранее определенным классам. \n",
    "\n",
    "Например, у нас есть измерения нескольких ирисов. Допустим - это длина лепестка и ширина лепестка. По этим измерениям надо отнести цветок к одному из видов:  Ирис щетинистый (Iris setosa), Ирис виргинский (Iris virginica) и Ирис разноцветный (Iris versicolor). Это пример носит названия ирисы Фишера. \n",
    "\n",
    "Ирис щетинистый (Iris setosa)  | Ирис виргинский (Iris virginica) | Ирис разноцветный (Iris versicolor)\n",
    "- | - | - \n",
    "![alt](img\\Irissetosa1.jpg) | ![alt](img\\736px-Iris_virginica.jpg) | ![alt](img\\800px-Iris_versicolor_3.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте получим данные и визуализируем их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification_helper import *\n",
    "X, y, X_unseen, y_unseen = get_data()\n",
    "plot_iris(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, допустим, у нас есть 3 новых сэмпла данных, которые мы хотим классифицировать (это желтые круг, треугольник и звезда). Попробуем вручную определить к какому классу они относятся. Как вы выбирали решение?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iris(X, y, X_unseen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# K ближайших соседей (kNN)\n",
    "\n",
    "Это один из самых простых методов в машинном обучении. Алгоритм можно описать следующим образом:\n",
    "\n",
    "1. Считаем расстояние от неизвестного сэмпла до данных в обучающем наборе.\n",
    "2. Находим $k$ ближайших сэмлов.\n",
    "3. Смотрим к какому классу относятся соседи и на основании большинства выбираем нужный класс. \n",
    "\n",
    "#### Плюсы\n",
    "* Простой в понимании и реализации;\n",
    "* Не требует обучения;\n",
    "* На простых данных показывает неплохие результаты.\n",
    "\n",
    "#### Минусы\n",
    "* Медленный. Если у нас в тестовом наборе миллион точек, то для каждой неизвестной точки требуется миллион раз рассчитать расстояние;\n",
    "* Не самый точный;\n",
    "* Проблема выбора числа соседей;\n",
    "* Проблема выбора метрики для расстояния;\n",
    "* Проклятие размерности.\n",
    "\n",
    "В качестве метрики расстояния обычно выбирается Евклидово расстояние, но есть и более экзотичные примеры (расстояние Хэмминага, например).\n",
    "$$ a = (x_a, y_a), b= (x_b, y_b)$$\n",
    "$$euclidian(a, b) = \\sqrt{ (x_a - x_b)^2 + (y_a - y_b)^2}$$\n",
    "$$hemming(a, b) = |(x_a - x_b)| + |(y_a - y_b)|$$\n",
    "Теперь давайте визуализируем результаты алгоритма. Цветные области означают границы классов. То есть, если точка попадает в красную область, то она классифицируется как Iris-Setosa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choose_knn(X, y, X_unseen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Но вот в чем проблема. Давайте теперь визуализируем данные сделав диапазон по осям x и y одинаковым."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iris(X, y, X_unseen=None, classifier=None, same_range=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Диапазон по оси x от 1 до 7, а диапазон по оси y от 0 до 3. И если мы меряем расстояние, то основной вклад в расстояние вносится осью x. Для решения этой проблемы данные нужно приводить к одному и тому же диапазону.\n",
    "Например, можно использовать нормализацию.\n",
    "\n",
    "$$X_i = \\frac{X_i - min(X)}{max(X) - min(X)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_norm, X_unseen_norm = normilize_data(X, X_unseen)\n",
    "plot_iris(X_norm, y, X_unseen_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь посмотрим, как наша модель ведет себя на нормализованных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choose_knn(X_norm, y, X_unseen_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Логистическая регрессия (для одного параметра)\n",
    "\n",
    "### Бинарная классификация\n",
    "\n",
    "Логистическая регрессия - это бинарный классификатор. То есть, классификатор, который работает только с двумя классами. \n",
    "\n",
    "### Что такое сигмоида\n",
    "\n",
    "Логистическая регрессия построена на функции, которая называется сигмоида. \n",
    "Сигмоида это функция вида $\\phi(z) = \\frac{1}{1 + e^{-z}}$, где z это результат какой-нибудь функции. Например, $z = \\theta_1x + \\theta_0$, где $\\theta_1$ и $\\theta_0$ числа.\n",
    "Где $e$ - это число Эйлера (числено равно 2.71...)\n",
    "В следующей ячейке находится график этой функции.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact_plot_sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "В следующей ячейке мы посмотрим, как можно классифицировать данные, с помощью этой функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sigmoid(theta1=1, theta0=0, plot_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Функция потерь (ошибки) \n",
    "\n",
    "Так как значение сигмоида находится между 0 и 1, а сами классы кодируются числом 0 и 1, мы может внести следующую функцию потерь.\n",
    "\n",
    "$J(\\mathbf{\\theta}) = \\frac{1}{2N} \\sum^N_{i=0} (\\phi(\\theta_1X_i + \\theta_0) - y)^2$, где $\\theta = \\begin{pmatrix}\n",
    "\\theta_0 \\\\\n",
    "\\theta_1 \\\\\n",
    "\\end{pmatrix}$\n",
    "\n",
    "Давайте визуализируем ошибку.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_simple_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Введем другую функцию потерь.\n",
    "\n",
    "$J(\\mathbf{\\theta}) = \\frac{1}{2N} \\sum^N_{i=0} (-y \\log{(\\phi(z_i))} - (1-y)\\log{(1-\\phi(z_i))}$, где $z_i = \\theta_1X_i + \\theta_0$\n",
    "\n",
    "$log$ - это логарифм. Он определяется как $log_a(b) = c$, когда $a^c = b$.\n",
    "\n",
    "Запустим следующую ячейку что бы получше понять новую функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_simg_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_simple_error(loss=J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Логистическая регрессия \n",
    "\n",
    "Вообще функция логистической регрессии может работать не только когда у нас есть один параметр. Для просторы мы рассмотрим пример с 2-мя параметронами. Данный пример можно легко расширить на большее количество примеров.\n",
    "\n",
    "И как мы говорили ранее, данные записаны в матрице $X$, размером $NxM$, где строка - это сэмпл данных, а колонка - это параметр. Обычно, для удобства, первая колонка данных - это единицы. Поэтому у нас будет матрица размера Nx3.\n",
    "\n",
    "Пусть данные записаны в матрице X.\n",
    "\n",
    "$\\begin{equation*}\n",
    "\\mathbf{X} = \n",
    "\\begin{pmatrix}\n",
    "1 & X_{0, 1} & X_{0, 2} \\\\\n",
    "1 & X_{1, 1} & X_{1, 2} \\\\\n",
    "\\cdots & \\cdots & \\cdots \\\\\n",
    "1 & X_{N-1, 1} & X_{N-1, 2} \\\\\n",
    "\\end{pmatrix}\n",
    "\\end{equation*}$\n",
    "\n",
    "Напоминаю, что $X_{i, j}$ - это элемент матрицы, который находится на i-ой строке и j-том столбце. Или проще говоря это j-ый параметр i-го сэмпла.\n",
    "\n",
    "И пусть у нас есть вектором значений тета.\n",
    "\n",
    "$\\begin{equation*}\n",
    "\\mathbf{\\theta} = \n",
    "\\begin{pmatrix}\n",
    "\\theta_0 \\\\\n",
    "\\theta_1 \\\\\n",
    "\\theta_2 \\\\\n",
    "\\end{pmatrix}\n",
    "\\end{equation*}$ \n",
    "\n",
    "Тогда функция логистической регрессии будет следующей:\n",
    "\n",
    "$\\phi(z_i)=\\frac{1}{1 + e^{-z_i}}, \\\\ z_i=\\theta_0 + \\theta_{1}X_{i, 1} + \\theta_{2}X_{i, 2}$\n",
    "\n",
    "\n",
    "Пусть\n",
    "$$Z = \\begin{pmatrix}\n",
    "z_0 \\\\\n",
    "z_1 \\\\\n",
    "\\cdots \\\\\n",
    "z_{N-2} \\\\\n",
    "z_{N-1} \\\\\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "Мы можем спокойно найти его перемножив матрицу $X$ и вектор $\\theta$.\n",
    "\n",
    "$$Z = X \\cdot \\theta = \n",
    "\\begin{pmatrix}\n",
    "1 & X_{0, 1} & X_{0, 2}  \\\\\n",
    "1 & X_{1, 1} & X_{1, 2}  \\\\\n",
    "\\cdots & \\cdots & \\cdots  \\\\\n",
    "1 & X_{N-2, 1} & X_{N-2, 2}  \\\\\n",
    "1 & X_{N-1, 1} & X_{N-1, 2}  \\\\\n",
    "\\end{pmatrix} \\cdot \n",
    "\\begin{pmatrix}\n",
    "\\theta_0 \\\\\n",
    "\\theta_1 \\\\\n",
    "\\theta_2 \\\\\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "1 \\cdot \\theta_0 +\\theta_1 X_{0, 1} + \\theta_2 X_{0, 2} \\\\\n",
    "1 \\cdot \\theta_0 +\\theta_1 X_{1, 1} + \\theta_2 X_{1, 2} \\\\\n",
    "\\cdots \\\\\n",
    "1 \\cdot \\theta_0 +\\theta_1 X_{N-2, 1} + \\theta_2 X_{N-2, 2}  \\\\\n",
    "1 \\cdot \\theta_0 +\\theta_1 X_{N-1, 1} + \\theta_2 X_{N-1, 2}  \\\\\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "\\theta_0 +\\theta_1 X_{0, 1} + \\theta_2 X_{0, 2} \\\\\n",
    "\\theta_0 +\\theta_1 X_{1, 1} + \\theta_2 X_{1, 2} \\\\\n",
    "\\cdots \\\\\n",
    "\\theta_0 +\\theta_1 X_{N-2, 1} + \\theta_2 X_{N-2, 2} \\\\\n",
    "\\theta_0 +\\theta_1 X_{N-1, 1} + \\theta_2 X_{N-1, 2} \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Таким образом:\n",
    "\n",
    "$$Z = \\begin{pmatrix}\n",
    "z_0 \\\\\n",
    "z_1 \\\\\n",
    "\\cdots \\\\\n",
    "z_{N-2} \\\\\n",
    "z_{N-1} \\\\\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "\\theta_0 +\\theta_1 X_{0, 1} + \\theta_2 X_{0, 2} \\\\\n",
    "\\theta_0 +\\theta_1 X_{1, 1} + \\theta_2 X_{1, 2} \\\\\n",
    "\\cdots \\\\\n",
    "\\theta_0 +\\theta_1 X_{N-2, 1} + \\theta_2 X_{N-2, 2} \\\\\n",
    "\\theta_0 +\\theta_1 X_{N-1, 1} + \\theta_2 X_{N-1, 2} \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Функция потери остается такой же.\n",
    "\n",
    "Для нахождения этих коэффициентов также используем градиентный спуск, как и в линейной регрессии. Но теперь нам необходимо найти производную от функции ошибки для каждого коэффициента. \n",
    "\n",
    "Таким образом сам алгоритм градиентного спуска можно описать следующим образом.\n",
    "\n",
    "* Выбираем случайное значение для $\\theta$\n",
    "* Повторить пока не сойдется:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\theta_{0_{new}} = \\theta_0 - \\alpha \\frac{\\delta J(\\theta)}{\\delta \\theta_0} = \\theta_0 - \\alpha \\frac{1}{N}\\sum_{i=0}^{N} (\\phi(z_i) - y_i)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\theta_{1_{new}} = \\theta_1 - \\alpha \\frac{\\delta J(\\theta)}{\\delta \\theta_1} = \\theta_1 - \\alpha \\frac{1}{N}\\sum_{i=0}^{N} (\\phi(z_i) - y_i)X_{i, 1}$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\theta_{2_{new}} = \\theta_2 - \\alpha \\frac{\\delta J(\\theta)}{\\delta \\theta_2} = \\theta_2 - \\alpha \\frac{1}{N}\\sum_{i=0}^{N} (\\phi(z_i) - y_i)X_{i, 2}$\n",
    "\n",
    "С помощью матрицы этот алгоритм можно переписать проще. \n",
    "\n",
    "$\\mathbf{\\nabla \\theta}$ будем назвать следующий вектор:\n",
    "\n",
    "$\\begin{equation*}\n",
    "\\mathbf{\\nabla \\theta} = \n",
    "\\begin{pmatrix}\n",
    "\\frac{\\delta J(\\theta)}{\\delta \\theta_0} \\\\\n",
    "\\frac{\\delta J(\\theta)}{\\delta \\theta_1} \\\\\n",
    "\\frac{\\delta J(\\theta)}{\\delta \\theta_2} \\\\\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix}\n",
    "\\frac{1}{N}\\sum_{i=0}^{N} (\\phi(z_i) - y_i) \\\\\n",
    "\\frac{1}{N}\\sum_{i=0}^{N} (\\phi(z_i) - y_i)X_{i, 1} \\\\\n",
    "\\frac{1}{N}\\sum_{i=0}^{N} (\\phi(z_i) - y_i)X_{i, 2} \\\\\n",
    "\\end{pmatrix}\n",
    "\\end{equation*}$\n",
    "\n",
    "Тогда:\n",
    "\n",
    "$\\begin{equation*}\n",
    "\\theta - \\alpha \\mathbf{\\nabla \\theta} = \n",
    "\\begin{pmatrix}\n",
    "\\theta_0 \\\\\n",
    "\\theta_1 \\\\\n",
    "\\theta_2 \\\\\n",
    "\\end{pmatrix}-\n",
    "\\alpha  \\begin{pmatrix}\n",
    "\\frac{\\delta J(\\theta)}{\\delta \\theta_0} \\\\\n",
    "\\frac{\\delta J(\\theta)}{\\delta \\theta_1} \\\\\n",
    "\\frac{\\delta J(\\theta)}{\\delta \\theta_2} \\\\\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix}\n",
    "\\theta_0 \\\\\n",
    "\\theta_1 \\\\\n",
    "\\theta_2 \\\\\n",
    "\\end{pmatrix}-\n",
    "\\begin{pmatrix}\n",
    "\\alpha  \\frac{\\delta J(\\theta)}{\\delta \\theta_0} \\\\\n",
    "\\alpha  \\frac{\\delta J(\\theta)}{\\delta \\theta_1} \\\\\n",
    "\\alpha  \\frac{\\delta J(\\theta)}{\\delta \\theta_2} \\\\\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix}\n",
    "\\theta_0 - \\alpha  \\frac{\\delta J(\\theta)}{\\delta \\theta_0} \\\\\n",
    "\\theta_1 - \\alpha  \\frac{\\delta J(\\theta)}{\\delta \\theta_1} \\\\\n",
    "\\theta_2 - \\alpha  \\frac{\\delta J(\\theta)}{\\delta \\theta_2} \\\\\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix}\n",
    "\\theta_0 - \\alpha  \\frac{1}{N}\\sum_{i=0}^{N} (\\phi(z_i) - y_i) \\\\\n",
    "\\theta_1 - \\alpha  \\frac{1}{N}\\sum_{i=0}^{N} (\\phi(z_i) - y_i)X_{i, 1} \\\\\n",
    "\\theta_2 - \\alpha  \\frac{1}{N}\\sum_{i=0}^{N} (\\phi(z_i) - y_i)X_{i, 2} \\\\\n",
    "\\end{pmatrix}\n",
    "\\end{equation*}$\n",
    "\n",
    "И теперь перепишем наш алгоритм:\n",
    "\n",
    "* Выбираем случайное значение для $\\theta$\n",
    "* Повторить пока не сойдется:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\theta_{new} = \\theta - \\alpha \\mathbf{\\nabla \\theta}$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\theta = \\theta_{new}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Использование бинарного классификатора, для многовариантной  классификаии\n",
    "\n",
    "Для этого используется метод один против всех. Допустим, у нас есть классы 1, 2, 3.\n",
    "\n",
    "Тогда для начала тренируется бинарный классификатор для класса 1, остальные классы помечаются как 0. Затем тренирутся второй классификатор для класса 2, остальные классы помечаются как 0. И так далее. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multy_log(X_norm, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Метод опорных векторов\n",
    "\n",
    "Бинарный классификатор, который старается построить максимальный зазор между данными. Это один из самых популярных методов.\n",
    "![alt](img\\SVM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Деревья решений\n",
    "\n",
    "Деревья решений проще проиллюстрировать, чем объяснить. Само дерево строится в помощью функции прироста информации (алгоритмы ID3 и C4.5).\n",
    "\n",
    "![alt](img\\tree.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Задания\n",
    "\n",
    "В качестве задания мы реализуем логистическую регрессию. В качестве входных данных у нас будет реальные данные опухолей рака груди.\n",
    "\n",
    "Матрица данных X содержит 3 параметра и 50 сэмплэв. То есть, ее размер 50x3. Нулевая колонка матрицы полностью состоит из единиц. Она нужна нам для удобства. Первая колонка содержит средний радиус опухоли, вторая колонка содержит среднее значение \"плавности\" опухоли. \n",
    "\n",
    "Вектор y размера 50x1. Если Сэмпл равен 1, то опухоль злокачественная, если 0 то доброкачественная.\n",
    "\n",
    "\n",
    "Нужно выполнить следующие задания\n",
    "\n",
    "1. Реализовать функцию сигмоиды;\n",
    "2. Реализовать функцию потерь;\n",
    "3. Рассчитать градиент;\n",
    "4. Реализовать градиентный спуск.\n",
    "\n",
    "Задания следует делать одно за другим.\n",
    "\n",
    "Для начала запустим следующую ячейку, которая визуализирует наши данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, = get_data_for_task()\n",
    "simple_plot(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализовать функцию сигмоиды\n",
    "\n",
    "На вход функции передается вектор $\\mathbf{\\theta}$ (размера 3x1), и матрица $\\mathbf{X}$ размера Nx3. Функция должна  возвращать вектор-столбец $\\mathbf{y}$ размера Nx1. \n",
    "   \n",
    "$\\begin{equation*}\n",
    "\\mathbf{\\theta} = \n",
    "\\begin{pmatrix}\n",
    "\\theta_0 \\\\\n",
    "\\theta_1 \\\\\n",
    "\\theta_2 \\\\\n",
    "\\end{pmatrix}\n",
    "\\end{equation*}$ \n",
    "\n",
    "$\\begin{equation*}\n",
    "\\mathbf{X} = \n",
    "\\begin{pmatrix}\n",
    "1 & X_{0, 1} & X_{0, 2} \\\\\n",
    "1 & X_{1, 1} & X_{1, 2} \\\\\n",
    "\\cdots & \\cdots & \\cdots \\\\\n",
    "1 & X_{N-1, 1} & X_{N-1, 2} \\\\\n",
    "\\end{pmatrix}\n",
    "\\end{equation*}$\n",
    "\n",
    "Подсказка: попробуйте умножить матрицу $X$ на вектор $\\theta$\n",
    "\n",
    "Требуется реализовать функцию $\\phi(z_i) = \\frac{1}{1+e^{-z_i}}$, где  $z_i=\\theta_0 + \\theta_{1}X_{i, 1} + \\theta_{2}X_{i, 2}$.\n",
    "Выходной вектор должен иметь следующие значения:\n",
    "\n",
    "$\\begin{equation*}\n",
    "\\mathbf{y} = \n",
    "\\begin{pmatrix}\n",
    "\\phi(\\theta_0 + \\theta_1 X_{0, 1} + \\theta_2 X_{0, 2}) \\\\\n",
    "\\phi(\\theta_0 + \\theta_1 X_{1, 1} + \\theta_2 X_{1, 2}) \\\\\n",
    "\\cdots \\\\\n",
    "\\phi(\\theta_0 + \\theta_1 X_{N-1, 1} + \\theta_2 X_{N-1, 2}) \\\\\n",
    "\\end{pmatrix}\n",
    "\\end{equation*}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(theta, X):\n",
    "    # Напоминаю, что код нужно писать в области между звездочками. \n",
    "    # Вы можете решить данную задачу с помощью цикла, но \n",
    "    # постарайтесь решить ее с помощью векторизации. \n",
    "    # Что бы получить вектор экспонент воспользуйтесь np.exp(z)\n",
    "    # Помни что в X[:, 0] содержаться единицы. \n",
    "\n",
    "    y = np.zeros((X.shape[0],)) # создаем переменную y и заполняем ее нулями\n",
    "    N = X.shape[0]       # получаем размер вектора столбца\n",
    "\n",
    "    #***********************************************************************\n",
    "\n",
    "    #***********************************************************************\n",
    "\n",
    "    return y.reshape(-1)\n",
    " \n",
    "check_sigmoid(sigmoid, X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализовать функцию потерь \n",
    "\n",
    "$J(\\mathbf{\\theta}) = \\frac{1}{2N} \\sum^N_{i=0} (-y \\log{(\\phi(z_i))} - (1-y)\\log{(1-\\phi(z_i))}$, где $z_i = \\theta_0 + \\theta_1X_{i,1} + \\theta_2 X_{i,2}   $\n",
    "\n",
    "На вход функции передается вектор $\\mathbf{\\theta}$ (размера 3x1), матрица $\\mathbf{X}$ размера Nx3 и вектор-столбец $\\mathbf{y}$ c с реальными значениями размера Nx1. Функция должна возвращать действительное число равное $J(\\theta)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(theta, X, y):\n",
    "    # Ты можешь использовать функцию sigmoid из предыдущего задания.\n",
    "    # Не бойся создавать новые переменные. \n",
    "    # Для того, что бы получить логарифм от вектора воспользуйся np.log()\n",
    "    \n",
    "    # Результат функции потерь должен быть записан в переменную J\n",
    "\n",
    "    N = X.shape[0]       # получаем размер вектора столбца\n",
    "    \n",
    "    J = 0\n",
    "\n",
    "    #***********************************************************************\n",
    "    \n",
    "\n",
    "    \n",
    "    #***********************************************************************\n",
    "\n",
    "    return J\n",
    " \n",
    "check_loss_func(loss_func, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализовать градиент\n",
    "\n",
    "Таким образом сам алгоритм градиентного спуска можно описать следующим образом.\n",
    "\n",
    "\n",
    "На вход функции передается вектор $\\mathbf{\\theta}$ (размера 3x1), матрица $\\mathbf{X}$ размера Nx3 и вектор-столбец $\\mathbf{y}$ c с реальными значениями размера Nx1. Функция должна возвращать вектор равный градиенту теты.\n",
    "\n",
    "$\\begin{equation*}\n",
    "\\mathbf{\\nabla \\theta} = \n",
    "\\begin{pmatrix}\n",
    "\\frac{\\delta J(\\theta)}{\\delta \\theta_0} \\\\\n",
    "\\frac{\\delta J(\\theta)}{\\delta \\theta_1} \\\\\n",
    "\\frac{\\delta J(\\theta)}{\\delta \\theta_2} \\\\\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix}\n",
    "\\frac{1}{N}\\sum_{i=0}^{N} (\\phi(z_i) - y_i) \\\\\n",
    "\\frac{1}{N}\\sum_{i=0}^{N} (\\phi(z_i) - y_i)X_{i, 1} \\\\\n",
    "\\frac{1}{N}\\sum_{i=0}^{N} (\\phi(z_i) - y_i)X_{i, 2} \\\\\n",
    "\\end{pmatrix}\n",
    "\\end{equation*}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_function(theta, X, y):\n",
    "    # Ты можешь использовать функцию sigmoid из предыдущего задания. \n",
    "    # Не бойся создавать новые переменные. \n",
    "    # Помни что ты можешь почлено перемножать вектора. \n",
    "    \n",
    "    # Результат функции потерь должен быть записан в вектор grad\n",
    "    # Помни что у тебя должно быть 3 значений градиента, для каждой теты.\n",
    "        \n",
    "    N = X.shape[0]       # получаем размер вектора столбца\n",
    "    grad = np.zeros((3, 1))\n",
    "\n",
    "    #***********************************************************************\n",
    "\n",
    "    \n",
    "    #***********************************************************************\n",
    "\n",
    "    return grad\n",
    "   \n",
    "check_gradient_function(gradient_function, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализовать градиентный спуск\n",
    "\n",
    "На вход функции передается вектор $\\mathbf{\\theta_{init}}$ (размера 3x1), матрица $\\mathbf{X}$ размера Nx3, вектор-столбец $\\mathbf{y}$ c с реальными значениями размера Nx1, параметр $\\alpha$ и число $iters$ равный количеству итераций в алгоритме. Функция должна возвращать вектор равный обновленной тете. Также функция возвращает значении функции потерь на каждой итерации.\n",
    "\n",
    "Для нахождения теты используем градиентный спуск, как и в линейной регрессии. Но теперь нам необходимо найти производную от функции ошибки для каждого коэффициента. \n",
    "\n",
    "Таким образом сам алгоритм градиентного спуска можно описать следующим образом.\n",
    "\n",
    "* Повторить $iters$ раз:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\theta_{new} = \\theta - \\alpha \\mathbf{\\nabla \\theta}$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\theta = \\theta_{new}$\n",
    "\n",
    "Где $\\mathbf{\\nabla \\theta}$ - это градиент найденный с помощью gradient_function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(init_theta, X, y, alpha, iters):\n",
    "    # Лучше всего использовать функцию gradient_function для \n",
    "    # нахождения градиента, чем пересчитывать его еще раз\n",
    "\n",
    "    # Такде не забудь посчитать и сохранить значение функции потерь \n",
    "    # для каждой итерации.\n",
    "\n",
    "    theta = init_theta\n",
    "    errors = [0]*iters\n",
    "\n",
    "    #***********************************************************************\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    #***********************************************************************\n",
    "\n",
    "    return theta, errors\n",
    "    \n",
    "check_gradient_descent(gradient_descent, X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
